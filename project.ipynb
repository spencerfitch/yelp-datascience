{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06703110-6b4a-4e2d-8a8c-c49dc5798b57",
   "metadata": {},
   "source": [
    "# CS396 Data Science: Project\n",
    "\n",
    "### Project Overview\n",
    "This project aims to understand any underlying biases or trends on Yelp by answering the following questions:\n",
    "1. Is there a correlation between a user's activity/popularity and their review score distribution?\n",
    "2. Do Yelp users have similar behaviors to their friends on the platform?\n",
    "3. Is there a correlation between review score and whether it was made during the business' operating hours?\n",
    "4. Is there a correlation between number of reviews on a business and its score distribution?\n",
    "\n",
    "### Data Used\n",
    "All of these questions were answered using various data sources included in the Yelp Open Dataset. The specific files used for each question are listed below:\n",
    "1. user.json, review.json\n",
    "2. user.json, review.json\n",
    "3. business.json, review.json\n",
    "4. business.json, review.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297cd5f4-b92d-4396-ac87-ca06004c8fc6",
   "metadata": {},
   "source": [
    "## Environment Setup and Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc9c0343-e6f0-4d66-ac8a-61eb600e27e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../../yelp_dataset/'\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"Load Yelp data file\n",
    "    Currently only JSON and pickle format supported\n",
    "    \n",
    "    function(string) => pd.DataFrame\"\"\"\n",
    "    f_type = path.split('.')[-1]\n",
    "    \n",
    "    if f_type == 'json': \n",
    "        return pd.DataFrame.from_records([json.loads(l) for l in open(DATA_PATH+path, encoding='utf-8')])\n",
    "    elif f_type in ['pickle', 'pkl']:\n",
    "        return pd.read_pickle(DATA_PATH+path)\n",
    "    else:\n",
    "        raise ValueError('Unsupported file type provided \"{0}\"'.format(f_type))\n",
    "        \n",
    "\n",
    "def save_data(df, path):\n",
    "    \"\"\"Save pd.DataFrame to specified file\n",
    "    Currently only JSON and pickle format supported\n",
    "    \n",
    "    function(pd.DataFrame, string) => None\"\"\"\n",
    "    f_type = path.split('.')[-1]\n",
    "    \n",
    "    if f_type == 'json':\n",
    "        with open(DATA_PATH+path, 'w', encoding='utf-8') as out:\n",
    "            for i, r in df.iterrows():\n",
    "                print(r.to_json(), file=out)\n",
    "        return\n",
    "    elif f_type in ['pickle', 'pkl']:\n",
    "        df.to_pickle(DATA_PATH+path)\n",
    "        return\n",
    "    else:\n",
    "        raise ValueError('Unsupported file type provied \"{0}\"'.format(f_type))\n",
    " \n",
    "\n",
    "def drop_null(df, subset=None):\n",
    "    \"\"\"Remove null values (or subset) and return statistics\n",
    "    \n",
    "    function(pd.DataFrame, List[String]) => pd.DataFrame, Number, Number\n",
    "    \"\"\"\n",
    "    # Remove null data\n",
    "    clean = df.dropna() if not subset else df.dropna(subset=subset)\n",
    "    \n",
    "    # Compute statistics\n",
    "    num_null = len(df) - len(clean)\n",
    "    percent_null = (num_null / len(df)) * 100\n",
    "    \n",
    "    return clean, num_null, percent_null       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ada3a7-6c34-4421-a123-02805800e1c6",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61db8255-445f-4e99-b6c4-fc5324fe5e59",
   "metadata": {},
   "source": [
    "### Convert JSON to Pickle\n",
    "All of the data files that will be used are converted to pickle format for significantly faster load times and performance. This process takes a long time to complete (>20 minutes on my system) and requires significant computational resources (>8 GB RAM usage), but saves time in the long run with significantly decreased loading times for later access. This also slightly increases data density, saving about 1 GB in total across the files transcoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8525ba62-2773-48ae-8a20-66f72c6737b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Commented out to prevent accidental execution\n",
    "files = ['yelp_academic_dataset_user', 'yelp_academic_dataset_review', 'yelp_academic_dataset_business']\n",
    "for f in files:\n",
    "    df = load_data(f+'.json')\n",
    "    save_data(df, f+'.pickle')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fed7c5-1309-4911-94b7-fd0ace24b4fb",
   "metadata": {},
   "source": [
    "### General Data Cleaning\n",
    "The cleaning for each dataset is handled by its own clean_{dataset} function. This allows for more precising cleaning to be added in the future in the event that it is needed. For the analysis that will be performed here, there is very little data cleaning that needs to be performed; aside from dropping null values, the user and business datasets need to be checked for duplicate user_id and business_id respectively. Upon analyzing the data used here, there were no such duplicates found, so the currently implementation of clean_{dataset} only serves as an alert of duplicate entries. There is no need to resolve any textual errors, such as an incorrect city name, as those will not be analyzed in this project and do not affect the other results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59c084a-ab3f-4751-8de2-bf078a50932f",
   "metadata": {},
   "source": [
    "#### User Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c194457d-9234-4029-94ba-96e4e7514ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_user(df):\n",
    "    \"\"\"Apply cleaning to user dataset or a subset of it\n",
    "    \n",
    "    function(pd.DataFrame) => pd.DataFrame\n",
    "    \"\"\"\n",
    "    print('=== User Data Cleaning Results ===')\n",
    "    \n",
    "    # Clean out entries with null values\n",
    "    clean, num_null, percent_null = drop_null(df)\n",
    "    print('  Null entries dropped:  {0} ({1}%)'.format(num_null, round(percent_null, 2)))\n",
    "    \n",
    "    # Ensure no duplicate user_ids\n",
    "    num_duplicates = len(clean) - len(clean.user_id.unique())\n",
    "    print('  Duplicate user_id:     {0}'.format(num_duplicates))\n",
    "    \n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5312e069-751f-43a8-b81e-754a6e12a95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== User Data Cleaning Results ===\n",
      "  Null entries dropped: 0 (0.0%)\n",
      "  Duplicate user_id:    0\n"
     ]
    }
   ],
   "source": [
    "df = load_data('yelp_academic_dataset_user.pickle')\n",
    "df = clean_user(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1661ef46-91d4-49ff-a55d-aa5fb8fb7567",
   "metadata": {},
   "source": [
    "#### Business Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a5abcc5-c4b2-466c-9893-c885789cb7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_business(df):\n",
    "    \"\"\"Apply cleaning to business dataset or a subset of it\n",
    "    \n",
    "    function(pd.DataFrame) => pd.DataFrame\n",
    "    \"\"\"\n",
    "    print(\"=== Business Data Cleaning Results ===\")\n",
    "    \n",
    "    # Clean out null values\n",
    "    clean, num_null, percent_null = drop_null(df, subset=['stars', 'review_count', 'is_open', 'hours'])\n",
    "    print('  Null entries dropped:   {0} ({1}%)'.format(num_null, round(percent_null, 2)))\n",
    "    \n",
    "    # Ensure no duplicate business_id's\n",
    "    num_duplicates = len(clean) - len(clean.business_id.unique())\n",
    "    print('  Duplicate business_id:  {0}'.format(num_duplicates))\n",
    "    \n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fc8e05d-2266-44b1-9357-c0c2a15d2be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Business Data Cleaning Results ===\n",
      "  Null entries dropped:   27341 (17.03%)\n",
      "  Duplicate business_id:  0\n"
     ]
    }
   ],
   "source": [
    "df = load_data('yelp_academic_dataset_business.pickle')\n",
    "df = clean_business(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a894bd-caff-45b6-bf71-8d6dddf4271d",
   "metadata": {},
   "source": [
    "#### Review Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b17c7119-1673-41f4-af5a-a1a1f2a1a0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_review(df):\n",
    "    \"\"\"Apply cleaning to review dataset or a subset of it\n",
    "    \n",
    "    function(pd.DataFrame) => pd.DataFrame\n",
    "    \"\"\"\n",
    "    print(\"=== Review Data Cleaning Results ===\")\n",
    "    \n",
    "    # Clean out null values\n",
    "    clean, num_null, percent_null = drop_null(df)\n",
    "    print('  Null entries dropped:  {0} ({1}%)'.format(num_null, round(percent_null, 2)))\n",
    "    \n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "218f90af-54fb-4672-b857-73cea3cdcb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Review Data Cleaning Results ===\n",
      "  Null entries dropped:  0 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "df = load_data('yelp_academic_dataset_review.pickle')\n",
    "df = clean_review(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2388adab-0876-4127-ad9a-7d77dbb82b20",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e70dad-ec17-4ac4-9de4-8fed5f374bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
