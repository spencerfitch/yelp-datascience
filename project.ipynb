{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06703110-6b4a-4e2d-8a8c-c49dc5798b57",
   "metadata": {},
   "source": [
    "# CS396 Data Science: Project\n",
    "\n",
    "### Project Overview\n",
    "This project aims to understand any underlying biases or trends on Yelp by answering the following questions:\n",
    "1. Is there a correlation between a user's activity/popularity and their review score distribution?\n",
    "2. Do Yelp users have similar behaviors to their friends on the platform?\n",
    "3. Is there a correlation between review score and whether it was made during the business' operating hours?\n",
    "4. Is there a correlation between number of reviews on a business and its score distribution?\n",
    "\n",
    "### Data Used\n",
    "All of these questions were answered using various data sources included in the Yelp Open Dataset. The specific files used for each question are listed below:\n",
    "1. user.json, checkin.json, review.json\n",
    "2. user.json, checkin.json, review.json\n",
    "3. business.json, review.json\n",
    "4. business.json, review.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297cd5f4-b92d-4396-ac87-ca06004c8fc6",
   "metadata": {},
   "source": [
    "## Environment Setup and Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dc9c0343-e6f0-4d66-ac8a-61eb600e27e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../../yelp_dataset/'\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"Load Yelp data file\n",
    "    Currently only JSON and pickle format supported\n",
    "    \n",
    "    function(string) => pd.DataFrame\"\"\"\n",
    "    f_type = path.split('.')[-1]\n",
    "    \n",
    "    if f_type == 'json': \n",
    "        return pd.DataFrame.from_records([json.loads(l) for l in open(DATA_PATH+path, encoding='utf-8')])\n",
    "    elif f_type in ['pickle', 'pkl']:\n",
    "        return pd.read_pickle(DATA_PATH+path)\n",
    "    else:\n",
    "        raise ValueError('Unsupported file type provided \"{0}\"'.format(f_type))\n",
    "        \n",
    "\n",
    "def save_data(df, path):\n",
    "    \"\"\"Save pd.DataFrame to specified file\n",
    "    Currently only JSON and pickle format supported\n",
    "    \n",
    "    function(pd.DataFrame, string) => None\"\"\"\n",
    "    f_type = path.split('.')[-1]\n",
    "    \n",
    "    if f_type == 'json':\n",
    "        with open(DATA_PATH+path, 'w', encoding='utf-8') as out:\n",
    "            for i, r in df.iterrows():\n",
    "                print(r.to_json(), file=out)\n",
    "        return\n",
    "    elif f_type in ['pickle', 'pkl']:\n",
    "        df.to_pickle(DATA_PATH+path)\n",
    "        return\n",
    "    else:\n",
    "        raise ValueError('Unsupported file type provied \"{0}\"'.format(f_type))\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ada3a7-6c34-4421-a123-02805800e1c6",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61db8255-445f-4e99-b6c4-fc5324fe5e59",
   "metadata": {},
   "source": [
    "#### Convert JSON to Pickle\n",
    "All of the data files that will be used are converted to pickle format for significantly faster load times and performance. This process takes a long time to complete (>20 minutes on my system) and requires significant computational resources (>8 GB RAM usage), but saves time in the long run with significantly decreased loading times for later access. This also slightly increases data density, saving about 1 GB in total across the files transcoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8525ba62-2773-48ae-8a20-66f72c6737b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Commented out to prevent accidental execution\n",
    "files = ['yelp_academic_dataset_user', 'yelp_academic_dataset_checkin', 'yelp_academic_dataset_review', 'yelp_academic_dataset_business']\n",
    "for f in files:\n",
    "    df = load_data(f+'.json')\n",
    "    save_data(df, f+'.pickle')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59c084a-ab3f-4751-8de2-bf078a50932f",
   "metadata": {},
   "source": [
    "#### User Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c194457d-9234-4029-94ba-96e4e7514ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_user(df):\n",
    "    \"\"\"Apply cleaning to user dataset or a subset of it\n",
    "    \n",
    "    function(pd.DataFrame) => pd.DataFrame\n",
    "    \"\"\"\n",
    "    print('=== User Data Cleaning Results ===')\n",
    "    \n",
    "    # Clean out entries with null values\n",
    "    clean = df.dropna()\n",
    "    \n",
    "    # Ensure no duplicate user_ids\n",
    "    num_duplicates = len(clean) - len(clean.user_id.unique())\n",
    "    print('  {0} duplicate user_ids'.format(num_duplicates))\n",
    "    \n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5312e069-751f-43a8-b81e-754a6e12a95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== User Data Cleaning Results ===\n",
      "  0 duplicate user_ids\n"
     ]
    }
   ],
   "source": [
    "df = load_data('yelp_academic_dataset_user.pickle')\n",
    "df = clean_user(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1661ef46-91d4-49ff-a55d-aa5fb8fb7567",
   "metadata": {},
   "source": [
    "#### Business Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0a5abcc5-c4b2-466c-9893-c885789cb7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_business(df):\n",
    "    \"\"\"Apply cleaning to business dataset or a subset of it\n",
    "    \n",
    "    function(pd.DataFrame) => pd.DataFrame\n",
    "    \"\"\"\n",
    "    print(\"=== Business Data Cleaning Results ===\")\n",
    "    \n",
    "    # Clean out null values\n",
    "    clean = df.dropna()\n",
    "    \n",
    "    # Ensure no duplicate business_id's\n",
    "    num_duplicates = len(clean) - len(clean.business_id.unique())\n",
    "    print('{0} duplicate user_ids'.format(num_duplicates))\n",
    "    \n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3fc8e05d-2266-44b1-9357-c0c2a15d2be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Business Data Cleaning Results ===\n",
      "0 duplicate user_ids\n"
     ]
    }
   ],
   "source": [
    "df = load_data('yelp_academic_dataset_business.pickle')\n",
    "df = clean_business(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a894bd-caff-45b6-bf71-8d6dddf4271d",
   "metadata": {},
   "source": [
    "#### Checkin Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "218f90af-54fb-4672-b857-73cea3cdcb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data('yelp_academic_dataset_checkin.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "76c19859-52c6-49ab-92b2-48bfd6b7058e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--0r8K_AQ4FZfLsX3ZYRDA</td>\n",
       "      <td>2017-09-03 17:13:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>--0zrn43LEaB4jUWTQH_Bg</td>\n",
       "      <td>2010-10-08 22:21:20, 2010-11-01 21:29:14, 2010...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>--164t1nclzzmca7eDiJMw</td>\n",
       "      <td>2010-02-26 02:06:53, 2010-02-27 08:00:09, 2010...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>--2aF9NhXnNVpDV0KS3xBQ</td>\n",
       "      <td>2014-11-03 16:35:35, 2015-01-30 18:16:03, 2015...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>--2mEJ63SC_8_08_jGgVIg</td>\n",
       "      <td>2010-12-15 17:10:46, 2013-12-28 00:27:54, 2015...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138871</th>\n",
       "      <td>zzoUa7lyeM-qKPKFYSrAhg</td>\n",
       "      <td>2012-10-12 17:11:06, 2012-10-22 23:38:12, 2012...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138872</th>\n",
       "      <td>zzpmoTVq4yn86U7ArHyFBQ</td>\n",
       "      <td>2020-07-18 18:33:18, 2020-07-18 20:13:49, 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138873</th>\n",
       "      <td>zzqq8J7Pibxod1YcknlkWA</td>\n",
       "      <td>2014-08-29 00:00:54, 2014-10-23 19:00:58, 2017...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138874</th>\n",
       "      <td>zzwK-TJsCJX5wZrdtKemPg</td>\n",
       "      <td>2010-08-29 17:39:58, 2010-10-25 22:58:03, 2011...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138875</th>\n",
       "      <td>zzzKmD9Mj6WtJwJUhA_1dg</td>\n",
       "      <td>2010-12-25 22:17:04, 2011-05-14 03:25:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>138876 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   business_id  \\\n",
       "0       --0r8K_AQ4FZfLsX3ZYRDA   \n",
       "1       --0zrn43LEaB4jUWTQH_Bg   \n",
       "2       --164t1nclzzmca7eDiJMw   \n",
       "3       --2aF9NhXnNVpDV0KS3xBQ   \n",
       "4       --2mEJ63SC_8_08_jGgVIg   \n",
       "...                        ...   \n",
       "138871  zzoUa7lyeM-qKPKFYSrAhg   \n",
       "138872  zzpmoTVq4yn86U7ArHyFBQ   \n",
       "138873  zzqq8J7Pibxod1YcknlkWA   \n",
       "138874  zzwK-TJsCJX5wZrdtKemPg   \n",
       "138875  zzzKmD9Mj6WtJwJUhA_1dg   \n",
       "\n",
       "                                                     date  \n",
       "0                                     2017-09-03 17:13:59  \n",
       "1       2010-10-08 22:21:20, 2010-11-01 21:29:14, 2010...  \n",
       "2       2010-02-26 02:06:53, 2010-02-27 08:00:09, 2010...  \n",
       "3       2014-11-03 16:35:35, 2015-01-30 18:16:03, 2015...  \n",
       "4       2010-12-15 17:10:46, 2013-12-28 00:27:54, 2015...  \n",
       "...                                                   ...  \n",
       "138871  2012-10-12 17:11:06, 2012-10-22 23:38:12, 2012...  \n",
       "138872  2020-07-18 18:33:18, 2020-07-18 20:13:49, 2020...  \n",
       "138873  2014-08-29 00:00:54, 2014-10-23 19:00:58, 2017...  \n",
       "138874  2010-08-29 17:39:58, 2010-10-25 22:58:03, 2011...  \n",
       "138875           2010-12-25 22:17:04, 2011-05-14 03:25:18  \n",
       "\n",
       "[138876 rows x 2 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "57c193b9-7969-4d93-bd5d-d7ad4c483994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "business_id    --0r8K_AQ4FZfLsX3ZYRDA\n",
       "date              2017-09-03 17:13:59\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51999015-0f72-4075-b618-149ba4931a6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
